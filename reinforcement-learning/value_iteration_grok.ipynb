{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value Iteration 2 - Hung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the MDP parameters\n",
    "states = ['S0', 'S1', 'S2']  # State space\n",
    "actions = ['a0', 'a1', 'a2']  # Action space\n",
    "gamma = 0.9  # Discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transition probabilities and rewards (based on the diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition probabilities and rewards (based on the diagram)\n",
    "# Format: {state: {action: {next_state: (probability, reward)}}}\n",
    "transitions = {\n",
    "    'S0': {\n",
    "        'a0': {'S0': (0.7, 10), 'S1': (0.3, 0)},  # Reward +10 for S0->S0, 0 for S0->S1\n",
    "        'a1': {'S1': (1.0, 40)},  # Reward +40 for S0->S1\n",
    "        'a2': {'S1': (0.8, 0), 'S2': (0.2, 0)}  # No explicit reward shown, assuming 0\n",
    "    },\n",
    "    'S1': {\n",
    "        'a1': {'S1': (0.1, 0), 'S2': (0.8, -50), 'S0': (0.1, 0)},  # Reward -50 for S1->S2\n",
    "        'a2': {'S2': (1.0, 0)}  # No explicit reward shown, assuming 0\n",
    "    },\n",
    "    'S2': {}  # Terminal state, no actions (value = 0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 2 iterations\n",
      "Optimal Value Function:\n",
      "V(S0) = 40.00\n",
      "V(S1) = 0.00\n",
      "V(S2) = 0.00\n"
     ]
    }
   ],
   "source": [
    "# Initialize value function V(s) to zeros\n",
    "V = {state: 0.0 for state in states}\n",
    "\n",
    "# Number of iterations for convergence\n",
    "max_iterations = 1000\n",
    "theta = 0.0001  # Convergence threshold\n",
    "\n",
    "# Value Iteration\n",
    "for i in range(max_iterations):\n",
    "    new_V = V.copy()\n",
    "    for state in states:\n",
    "        if state == 'S2':  # Terminal state, value remains 0\n",
    "            continue\n",
    "        \n",
    "        # Calculate the value for each action and take the maximum\n",
    "        action_values = []\n",
    "        for action in actions:\n",
    "            if action in transitions[state]:\n",
    "                value = 0\n",
    "                for next_state, (prob, reward) in transitions[state][action].items():\n",
    "                    value += prob * (reward + gamma * V[next_state])\n",
    "                action_values.append(value)\n",
    "        \n",
    "        # If no valid actions, keep current value (e.g., for terminal state)\n",
    "        if action_values:\n",
    "            new_V[state] = max(action_values)\n",
    "    \n",
    "    # Check for convergence\n",
    "    if max(abs(new_V[state] - V[state]) for state in states) < theta:\n",
    "        print(f\"Converged after {i + 1} iterations\")\n",
    "        break\n",
    "    \n",
    "    V = new_V\n",
    "\n",
    "# Print the optimal value function\n",
    "print(\"Optimal Value Function:\")\n",
    "for state, value in V.items():\n",
    "    print(f\"V({state}) = {value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Policy:\n",
      "π(S0) = a1\n",
      "π(S1) = a2\n",
      "π(S2) = None\n"
     ]
    }
   ],
   "source": [
    "# Derive the optimal policy\n",
    "policy = {}\n",
    "for state in states:\n",
    "    if state == 'S2':\n",
    "        policy[state] = None  # Terminal state, no action\n",
    "        continue\n",
    "    \n",
    "    best_action = None\n",
    "    best_value = float('-inf')\n",
    "    for action in actions:\n",
    "        if action in transitions[state]:\n",
    "            value = 0\n",
    "            for next_state, (prob, reward) in transitions[state][action].items():\n",
    "                value += prob * (reward + gamma * V[next_state])\n",
    "            if value > best_value:\n",
    "                best_value = value\n",
    "                best_action = action\n",
    "    policy[state] = best_action\n",
    "\n",
    "# Print the optimal policy\n",
    "print(\"\\nOptimal Policy:\")\n",
    "for state, action in policy.items():\n",
    "    print(f\"π({state}) = {action}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
