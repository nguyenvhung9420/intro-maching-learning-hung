{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning Algoritm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /Users/hungnguy/Library/Python/3.9/lib/python/site-packages (1.26.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the MDP components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MDP components\n",
    "states = [\"s0\", \"s1\", \"s2\"]\n",
    "actions = [\"a0\", \"a1\", \"a2\"]\n",
    "gamma = 0.9  # Discount factor\n",
    "alpha = 0.1  # Learning rate\n",
    "epsilon = 0.1  # Exploration probability (for ε-greedy)\n",
    "episodes = 10000  # Number of episodes\n",
    "max_steps = 100  # Max steps per episode\n",
    "\n",
    "\n",
    "threshold = 1e-6  # Convergence threshold\n",
    "max_episodes = 100000  # Maximum number of episodes to avoid infinite loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transition probabilities and rewards\n",
    "Format: transition_probs[state][action] = [(next_state, probability, reward)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition probabilities and rewards\n",
    "# Format: transition_probs[state][action] = [(next_state, probability, reward)]\n",
    "transition_probs = {\n",
    "    \"s0\": {\n",
    "        \"a0\": [(\"s0\", 0.7, 10), (\"s1\", 0.3, 0)],\n",
    "        \"a1\": [(\"s0\", 1.0, 0)],\n",
    "        \"a2\": [(\"s1\", 0.8, 0), (\"s2\", 0.2, 0)],\n",
    "    },\n",
    "    \"s1\": {\n",
    "        \"a0\": [(\"s1\", 1.0, 0)],\n",
    "        \"a2\": [(\"s2\", 1.0, -50)],\n",
    "    },\n",
    "    \"s2\": {\n",
    "        \"a1\": [(\"s0\", 0.8, 40), (\"s2\", 0.1, 0), (\"s1\", 0.1, 0)],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-values\n",
    "Q = {state: {action: 0 for action in actions} for state in states}\n",
    "\n",
    "# Helper function to choose an action using ε-greedy\n",
    "def choose_action(state):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(actions)  # Explore\n",
    "    return max(Q[state], key=Q[state].get)  # Exploit\n",
    "\n",
    "# Helper function to simulate the environment\n",
    "def step(state, action):\n",
    "    if action not in transition_probs[state]:\n",
    "        return state, 0  # Invalid action, no reward\n",
    "    \n",
    "    transitions = transition_probs[state][action]\n",
    "    next_state, reward = None, None\n",
    "    prob = random.uniform(0, 1)\n",
    "    cumulative_prob = 0\n",
    "\n",
    "    for next_s, p, r in transitions:\n",
    "        cumulative_prob += p\n",
    "        if prob <= cumulative_prob:\n",
    "            next_state, reward = next_s, r\n",
    "            break\n",
    "\n",
    "    return next_state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Q-Learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning algorithm\n",
    "for episode in range(episodes):\n",
    "    state = random.choice(states)  # Start from a random state\n",
    "    for _ in range(max_steps):\n",
    "        action = choose_action(state)  # Choose an action\n",
    "        next_state, reward = step(state, action)  # Take the action and observe the outcome\n",
    "        \n",
    "        # Update the Q-value using the Q-Learning formula\n",
    "        best_next_action = max(Q[next_state], key=Q[next_state].get)\n",
    "        Q[state][action] += alpha * (reward + gamma * Q[next_state][best_next_action] - Q[state][action])\n",
    "        \n",
    "        state = next_state  # Move to the next state\n",
    "\n",
    "# Derive the optimal policy from the Q-values\n",
    "policy = {state: max(Q[state], key=Q[state].get) for state in states}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Q-Values:\n",
      "  s0: {'a0': 12.565564130069587, 'a1': 11.158058329381172, 'a2': 9.143247144497526}\n",
      "  s1: {'a0': 7.349250948221477e-15, 'a1': 7.296850220100894e-15, 'a2': -7.241827715716406}\n",
      "  s2: {'a0': 39.95592255673869, 'a1': 43.79011198574897, 'a2': 39.99955557944611}\n",
      "\n",
      "Optimal Policy:\n",
      "  s0: a0\n",
      "  s1: a0\n",
      "  s2: a1\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "print(\"Optimal Q-Values:\")\n",
    "for state in Q:\n",
    "    print(f\"  {state}: {Q[state]}\")\n",
    "\n",
    "print(\"\\nOptimal Policy:\")\n",
    "for state, action in policy.items():\n",
    "    print(f\"  {state}: {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Iteration (Convergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Learning Converged in 21 episodes.\n"
     ]
    }
   ],
   "source": [
    "# Q-Learning Algorithm\n",
    "def q_learning_convergence():\n",
    "    episode_count = 0\n",
    "    while episode_count < max_episodes:\n",
    "        episode_count += 1\n",
    "        state = random.choice(states)  # Start from a random state\n",
    "        max_q_change = 0  # Track maximum Q-value change in this episode\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            action = choose_action(state)  # Choose an action\n",
    "            next_state, reward = step(state, action)  # Take the action and observe the outcome\n",
    "            \n",
    "            # Update the Q-value using the Q-Learning formula\n",
    "            best_next_action = max(Q[next_state], key=Q[next_state].get)\n",
    "            old_q_value = Q[state][action]\n",
    "            Q[state][action] += alpha * (reward + gamma * Q[next_state][best_next_action] - Q[state][action])\n",
    "            max_q_change = max(max_q_change, abs(Q[state][action] - old_q_value))\n",
    "            \n",
    "            state = next_state  # Move to the next state\n",
    "\n",
    "        # Check for convergence\n",
    "        if max_q_change < threshold:\n",
    "            break\n",
    "\n",
    "    return episode_count\n",
    "\n",
    "# Run Q-Learning and calculate convergence\n",
    "q_learning_episodes = q_learning_convergence()\n",
    "print(f\"Q-Learning Converged in {q_learning_episodes} episodes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
